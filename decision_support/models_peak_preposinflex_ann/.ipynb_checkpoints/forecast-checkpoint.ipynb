{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resultados CNN1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import pearsonr\n",
    "from tensorflow import keras\n",
    "\n",
    "import ai_utils.metrics as ai_metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import joblib\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para exibir os resultados para cada horizonte de predição\n",
    "\n",
    "- y_true: DataFrame com os valores verdadeiros\n",
    "- y_pred: DataFrame com os valores preditos\n",
    "- title: Título para exibição\n",
    "- xlabel: Título para o eixo x\n",
    "- ylabel: Título para o eixo y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(y_true, y_pred, out, title='', xlabel='', ylabel=''):\n",
    "    \n",
    "    plt.figure(figsize=(14, 18))\n",
    "    plt.suptitle(title, fontsize=14, y=1.03)\n",
    "    \n",
    "    n_cols = y_true.shape[1]\n",
    "    \n",
    "    for col in range(n_cols):\n",
    "        \n",
    "        days = list(range(col+1, (col+1)+len(y_true)))\n",
    "        \n",
    "        plt.subplot((n_cols/4)+1, 4, col+1)\n",
    "        plt.title('Previsões para t+{}'.format(col+1))\n",
    "        plt.plot(days, y_true.iloc[:, col], label='Real')\n",
    "        plt.plot(days, y_pred.iloc[:, col], label='Previsão')\n",
    "        plt.legend()\n",
    "        plt.xlabel(xlabel, fontsize=10)\n",
    "        plt.ylabel(ylabel, fontsize=10)\n",
    "        plt.xlim(left=days[0], right=days[-1])\n",
    "        plt.xticks(np.linspace(days[0], days[-1], num=5, dtype=int))\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "    plt.savefig(out)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para exibir os resultados das métricas para cada horizonte de predição\n",
    "\n",
    "- y_true: DataFrame com os valores verdadeiros\n",
    "- y_pred: DataFrame com os valores preditos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(y_true, y_pred):\n",
    "    \n",
    "    metrics = pd.DataFrame(columns=['MSE', 'RMSE', 'MAE', 'Pearson r', 'Fac2'])\n",
    "    \n",
    "    n_cols = y_true.shape[1]\n",
    "    \n",
    "    for col in range(n_cols):\n",
    "        y_true_col = y_true.iloc[:, col]\n",
    "        y_pred_col = y_pred.iloc[:, col]\n",
    "\n",
    "        if y_true_col.name == y_pred_col.name:\n",
    "            col_name = y_true_col.name\n",
    "        else:\n",
    "            print('Column name error.')\n",
    "            return None\n",
    "\n",
    "        mse = mean_squared_error(y_true_col, y_pred_col)\n",
    "        rmse = mean_squared_error(y_true_col, y_pred_col, squared=False)\n",
    "        mae = mean_absolute_error(y_true_col, y_pred_col)\n",
    "        pearson_r = pearsonr(y_true_col, y_pred_col)[0]\n",
    "        fac2 = ai_metrics.fac2(y_true_col.values, y_pred_col.values, to_numpy=True)\n",
    "        metrics.loc[col_name, :] = [mse, rmse, mae, pearson_r, fac2]\n",
    "\n",
    "    metrics.loc['Average', :] = metrics.mean()\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para exibir os gráficos com o histórico das métricas\n",
    "\n",
    "- dict_metrics: Dict com as tabelas contendo as métricas\n",
    "- title: Título para exibição\n",
    "- xlabel: Título para o eixo x\n",
    "- ylabel: Título para o eixo y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(dict_metrics, out, title='', xlabel='', ylabel=''):\n",
    "    \n",
    "    plt.figure(figsize=(14, 12))\n",
    "    plt.suptitle(title, fontsize=14, y=0.935)\n",
    "    \n",
    "    keys = list(dict_metrics.keys())\n",
    "    key_0 = keys[0]\n",
    "    metrics_df_0 = dict_metrics[key_0]\n",
    "    metrics_names = metrics_df_0.columns\n",
    "    time_steps = list(range(1, metrics_df_0.shape[0]))\n",
    "    \n",
    "    for col, metric_name in enumerate(metrics_names):\n",
    "        \n",
    "        plt.subplot(3, 2, col+1)\n",
    "        \n",
    "        for key in dict_metrics.keys():\n",
    "            \n",
    "            metrics_df = dict_metrics[key]\n",
    "            plt.plot(time_steps, metrics_df.iloc[0:-1, col], marker='o', label=key)\n",
    "        \n",
    "        plt.title('{} para cada horizonte de previsão'.format(metric_name))\n",
    "        plt.legend()\n",
    "        plt.xlabel('Dias a frente', fontsize=10)\n",
    "        plt.ylabel(metric_name, fontsize=10)\n",
    "        plt.xlim(left=time_steps[0], right=time_steps[-1])\n",
    "        plt.xticks(np.linspace(time_steps[0], time_steps[-1], num=5, dtype=int))\n",
    "        \n",
    "    plt.subplots_adjust(wspace=0.2, hspace=0.4)\n",
    "    plt.savefig(out)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para exibir o scatter plot de cada horizonte de predição\n",
    "\n",
    "- title: Título para exibição\n",
    "- xlabel: Título para o eixo x\n",
    "- ylabel: Título para o eixo y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter(y_true, y_pred, out, title=''):\n",
    "    \n",
    "    plt.figure(figsize=(14, 18))\n",
    "    plt.suptitle(title, fontsize=14, y=1.03)\n",
    "    \n",
    "    n_cols = y_true.shape[1]\n",
    "    \n",
    "    for col in range(n_cols):\n",
    "        \n",
    "        max_val = max(y_true.iloc[:, col].max(), y_pred.iloc[:, col].max())\n",
    "        min_val = min(y_true.iloc[:, col].min(), y_pred.iloc[:, col].min())\n",
    "                \n",
    "        max_lim = max_val + max_val*0.05\n",
    "        max_lim = math.ceil(max_lim)\n",
    "        \n",
    "        ndigits = len(str(max_lim))\n",
    "        max_lim = int(math.ceil(max_lim / (10.0**(ndigits-2)) )) * (10.0**(ndigits-2))\n",
    "        \n",
    "        min_lim = min_val - max_val*0.05\n",
    "        min_lim = max(0, min_lim)\n",
    "        \n",
    "        plt.subplot((n_cols/4)+1, 4, col+1)\n",
    "        plt.title('Previsões para t+{}'.format(col+1))\n",
    "        plt.scatter(y_true.iloc[:, col], y_pred.iloc[:, col])\n",
    "        plt.xlabel('Real', fontsize=10)\n",
    "        plt.ylabel('Previsão', fontsize=10)\n",
    "        \n",
    "        plt.xlim(left=min_lim, right=max_lim)\n",
    "        plt.ylim(top=max_lim, bottom=min_lim)\n",
    "        \n",
    "        plt.xticks(np.linspace(min_lim, max_lim, num=5, dtype=int))\n",
    "        plt.yticks(np.linspace(min_lim, max_lim, num=5, dtype=int))\n",
    "\n",
    "        plt.plot([min_lim, max_lim], [min_lim, max_lim])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "    plt.savefig(out)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para preparar a estrutura de uma respectiva região do DataFrame para entrada na rede\n",
    "\n",
    "- key: chave considerada para agrupar, isoladamente, cada série\n",
    "- region_data: DataFrame com as séries\n",
    "- look_back: Tamanho do look back\n",
    "- look_forward: Tamanho para o look forward\n",
    "- x_columns: Colunas consideradas para entrada no modelo\n",
    "- y_columns: Colunas consideradas para o target do modelo\n",
    "- gen_x & gen_y: habilitam a geração de cada output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_region_dataset(key, region_data, look_back, look_forward, x_columns, y_columns=None, gen_x=True, gen_y=True):\n",
    "    \n",
    "    # Check region dataframe\n",
    "    if region_data is None:\n",
    "        print('generate_single_region_dataset error: Region data is None!')\n",
    "        return (None, None)\n",
    "    \n",
    "    # Check number of regions\n",
    "    if len(region_data[key].unique()) > 1:\n",
    "        print('generate_single_region_dataset error: More than one region in the dataframe!')\n",
    "        return (None, None)\n",
    "    else:\n",
    "        region_name = region_data[key].unique()[0]\n",
    "    \n",
    "    # Drop 'Region' column\n",
    "    region_data = region_data.drop(columns=key)\n",
    "    \n",
    "    # Check the number of samples available to\n",
    "    # generate the look back and look forward windows\n",
    "    if len(region_data) < (look_back + look_forward):\n",
    "        print('generate_single_region_dataset error: Not enough samples '+\n",
    "              'in {} to generate the windows!'.format(region_name))\n",
    "        return (None, None)\n",
    "    \n",
    "    n_samples = len(region_data) - look_back - look_forward + 1\n",
    "\n",
    "    var_names = x_columns\n",
    "    \n",
    "    # Generate inputs\n",
    "    if gen_x:\n",
    "        inputs = pd.DataFrame()\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            input_window = region_data.T.iloc[:, i:i+look_back]\n",
    "            wide_input_sample = pd.DataFrame()\n",
    "            \n",
    "            for var in var_names:\n",
    "                var_input_sample = input_window.loc[var:var, :]\n",
    "                var_input_sample.columns = ['{}_t{}'.format(var, a) for a in range(1-look_back, 1)]\n",
    "                var_input_sample = var_input_sample.reset_index(drop=True)\n",
    "                wide_input_sample = pd.concat([wide_input_sample, var_input_sample], axis='columns')\n",
    "                \n",
    "            inputs = pd.concat([inputs, wide_input_sample], axis='index')\n",
    "            \n",
    "        # Insert region name\n",
    "        #inputs.insert(loc=0, column=key, value=region_name)\n",
    "        # Reset index\n",
    "        inputs = inputs.reset_index(drop=True)\n",
    "\n",
    "    # Generate outputs\n",
    "    if gen_y:\n",
    "        \n",
    "        if y_columns is None:\n",
    "            print('generate_single_region_dataset error: Need to specify column labels!')\n",
    "            return (None, None)\n",
    "        \n",
    "        var_names = y_columns\n",
    "        outputs = pd.DataFrame()\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            output_window = region_data.T.iloc[:, i+look_back : i+look_back+look_forward]\n",
    "            wide_output_sample = pd.DataFrame()\n",
    "\n",
    "            for var in var_names:\n",
    "                var_output_sample = output_window.loc[var:var, :]\n",
    "                var_output_sample.columns = ['{}_t+{}'.format(var, a) for a in range(1, look_forward+1)]\n",
    "                var_output_sample = var_output_sample.reset_index(drop=True)\n",
    "                wide_output_sample = pd.concat([wide_output_sample, var_output_sample], axis='columns')\n",
    "\n",
    "            outputs = pd.concat([outputs, wide_output_sample], axis='index')\n",
    "        \n",
    "        # Insert region name\n",
    "        #outputs.insert(loc=0, column=key, value=region_name)\n",
    "        # Reset index\n",
    "        outputs = outputs.reset_index(drop=True)\n",
    "        \n",
    "    if gen_x and gen_y:\n",
    "        return (inputs, outputs)\n",
    "    elif gen_x:\n",
    "        return (inputs, None)\n",
    "    elif gen_y:\n",
    "        return (None, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para preparar a estrutura do DataFrame para entrada na rede\n",
    "\n",
    "- key: chave considerada para agrupar, isoladamente, cada série\n",
    "- region_data: DataFrame com as séries\n",
    "- look_back: Tamanho do look back\n",
    "- look_forward: Tamanho para o look forward\n",
    "- x_columns: Colunas consideradas para entrada no modelo\n",
    "- y_columns: Colunas consideradas para o target do modelo\n",
    "- gen_x & gen_y: habilitam a geração de cada output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regions_dataset(key, regions_data, look_back, look_forward, x_columns, y_columns=None, \n",
    "                             gen_x=True, gen_y=True):\n",
    "    \n",
    "    regions_names = regions_data[key].unique()\n",
    "    \n",
    "    all_regions_x = pd.DataFrame()\n",
    "    all_regions_y = pd.DataFrame()\n",
    "    \n",
    "    for region_name in regions_names:\n",
    "        \n",
    "        region_data = regions_data[regions_data[key]==region_name]\n",
    "        \n",
    "        region_x, region_y = generate_single_region_dataset(key,\n",
    "            region_data, look_back=look_back, look_forward=look_forward, x_columns=x_columns, y_columns=y_columns,\n",
    "            gen_x=gen_x, gen_y=gen_y\n",
    "        )\n",
    "        \n",
    "        if not (region_x is None):\n",
    "            all_regions_x = pd.concat([all_regions_x, region_x])\n",
    "            all_regions_x = all_regions_x.reset_index(drop=True)\n",
    "            \n",
    "        if not (region_y is None):\n",
    "            all_regions_y = pd.concat([all_regions_y, region_y])\n",
    "            all_regions_y = all_regions_y.reset_index(drop=True)\n",
    "    \n",
    "    if gen_x and gen_y:\n",
    "        return (all_regions_x, all_regions_y)\n",
    "    elif gen_x:\n",
    "        return (all_regions_x, None)\n",
    "    elif gen_y:\n",
    "        return (None, all_regions_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para exibir o gráfico histórico de loss do treinamento\n",
    "\n",
    "- filepath: caminho para o arquivo com o history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history_from_file(filepath, out):\n",
    "    \n",
    "    with open(filepath, 'rb') as f:\n",
    "        history = pickle.load(f)\n",
    "    \n",
    "    plt.figure(figsize=[10, 6])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.plot(range(1, len(history['loss'])+1), history['loss'], label='Train Loss')\n",
    "    plt.plot(range(1, len(history['loss'])+1), history['val_loss'], label = 'Validation loss')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.savefig(out)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para realizar a inferência\n",
    "\n",
    "- model: função para carregar o modelo ou o próprio modelo\n",
    "- lookback: lookback utilizado nos dados\n",
    "- build_model: False se um modelo for passado em `model`\n",
    "- x_train, y_train: None se o build_model for False\n",
    "- step_inference: total de step para previsão final após inferência\n",
    "- data_reshape_func: função caso seja feito algum reshape sobre os dados de teste\n",
    "- reshape_train, reshape_val: True caso deseje aplicar o mesmo reshape no conjunto de treinamento e validação\n",
    "- trainable: True caso deseje realizar um novo treinamento do modelo\n",
    "- step: tamanho atual do Look forward\n",
    "- x_val, y_val: dados do conjunto de validação\n",
    "- x_test, y_test: dados do conjunto de teste\n",
    "- shuffle, batch_size, epochs, verbose: utilizando para treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_inference(model, lookback, position, build_model=True, x_train= None, y_train = None, step_inference=30, data_reshape_func=None,\n",
    "                 reshape_train=False, reshape_val=False, trainable =True, step=10, x_val = None, y_val = None,\n",
    "                 x_test=None, y_test=None, shuffle=False, batch_size=10, epochs=10, verbose=1):\n",
    "    \n",
    "    if step_inference % step != 0:\n",
    "        print('Error - timesteps % inference steps != 0')\n",
    "        return None, None, None\n",
    "    \n",
    "    if build_model == True:\n",
    "        modelo = model()\n",
    "    else:\n",
    "        modelo = model\n",
    "    \n",
    "    if reshape_train == True:\n",
    "        x_train = data_reshape_func(x_train)\n",
    "    else:\n",
    "        pass\n",
    "        #print(\"Warning - You are not reshaping the x_train!\")\n",
    "    \n",
    "    history = None\n",
    "    if trainable == True:\n",
    "        if x_train is None or y_train is None:\n",
    "            print('Error - Trainable Model but no x_train or y_train')\n",
    "            return None, None, None\n",
    "        if x_val is not None and y_val is not None:\n",
    "            if reshape_val == True:\n",
    "                x_val = data_reshape_func(x_val)\n",
    "            else:\n",
    "                pass\n",
    "                #print(\"Warning - You are not reshaping the x_val!\")\n",
    "            history = modelo.fit(x_train, y_train, validation_data=(x_val, y_val), shuffle=shuffle, batch_size = batch_size, epochs = epochs, verbose=verbose)\n",
    "        else:\n",
    "            history = modelo.fit(x_train, y_train, shuffle=shuffle ,batch_size = batch_size, epochs = epochs, verbose=verbose)\n",
    "        \n",
    "    if x_test is not None:\n",
    "        step_list = [] #np.empty(int(y_train.shape[1] % step))\n",
    "        #predicao_desnormalizado = []\n",
    "        #print(x_test)\n",
    "        for w in range(int(step_inference / step)):\n",
    "            if w == 0:\n",
    "                next_step = np.copy(x_test)\n",
    "                new_x_test = np.copy(x_test)\n",
    "                if data_reshape_func is not None:\n",
    "                    new_x_test = data_reshape_func(x_test)\n",
    "                else:\n",
    "                    print(\"Warning - You are not reshaping the x_test!\")\n",
    "                \n",
    "                #print(new_x_test.shape)\n",
    "                step_list.append(modelo.predict(new_x_test))\n",
    "                #predicao_desnormalizado.append(normalizador_y.inverse_transform(predicao_t10))\n",
    "            else:\n",
    "                new_step = np.copy(x_test)\n",
    "                for i in range(new_step.shape[0]):\n",
    "                        if lookback <= step:\n",
    "                            for j in range(lookback):\n",
    "                                new_step[i, j+position] = step_list[w-1][i, -lookback + j]\n",
    "                        else:\n",
    "                            for j in range(step):\n",
    "                                new_step = np.copy(next_step)\n",
    "                                new_step[i, j+position+(lookback-step)] = step_list[w-1][i, -step + j]\n",
    "                                next_step[i, j+position+(lookback-(2*step))] = step_list[w-1][i, -step + j]\n",
    "                \n",
    "                new_x_test = np.copy(new_step)\n",
    "                if data_reshape_func is not None:\n",
    "                    new_x_test = data_reshape_func(new_step)\n",
    "                \n",
    "                step_list.append(modelo.predict(new_x_test))\n",
    "                #predicao_t20_desnormalizado = normalizador_y.inverse_transform(predicao_t20)\n",
    "        \n",
    "        predicao = None\n",
    "        for k in range(len(step_list) - 1):\n",
    "            if k == 0:\n",
    "                predicao = np.append(step_list[k], step_list[k+1], axis=1)\n",
    "            else:\n",
    "                predicao = np.append(predicao, step_list[k+1], axis=1)\n",
    "    \n",
    "        return history, predicao, modelo\n",
    "    \n",
    "    return history, None, modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variáveis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colunas dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COLS = [\n",
    "#     'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'Confirmed',\n",
    "#     'Country/Region', 'Date', 'Deaths', 'Recovered', 'cases_day', 'end',\n",
    "#     'estagio', 'grocery_and_pharmacy', 'parks', 'peak', 'postinflex',\n",
    "#     'preinflex', 'residential', 'retail_and_recreation', 'transit_stations',\n",
    "#     'workplaces'\n",
    "# ]\n",
    "# COLS_OHE = [\n",
    "#     'C1.0', 'C1.1', 'C2.0', 'C2.1', 'C3.0', 'C3.1', 'C4.0', 'C4.1', 'C5.0',\n",
    "#     'C5.1', 'C6.0', 'C6.1', 'C7.0', 'C7.1', 'C8.0', 'C8.1', 'Confirmed',\n",
    "#     'Country/Region', 'Date', 'Deaths', 'estagio', 'grocery_and_pharmacy',\n",
    "#     'parks', 'peak', 'postinflex', 'preinflex', 'residential',\n",
    "#     'retail_and_recreation', 'transit_stations', 'workplaces'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colunas a serem utilizadas para prever casos confirmados e mortes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS_WD = ['Deaths', 'Confirmed', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8']\n",
    "\n",
    "# COLS_WD = [\n",
    "#     'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'Confirmed',\n",
    "#     'Deaths', 'grocery_and_pharmacy', 'parks', 'residential',\n",
    "#     'retail_and_recreation', 'transit_stations', 'workplaces'\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caminho base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = 'results/peak_preposinflex/lb25_wdmc_new_relu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caminho para os DataFrame reestruturados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_XTRAIN = f'{BASE}/x_train.csv'\n",
    "F_YTRAIN = f'{BASE}/y_train.csv'\n",
    "\n",
    "F_XTEST = f'{BASE}/x_test.csv'\n",
    "F_YTEST = f'{BASE}/y_test.csv'\n",
    "\n",
    "F_XVAL = f'{BASE}/x_val.csv'\n",
    "F_YVAL = f'{BASE}/y_val.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caminho para o scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_XSCALER = f'{BASE}/x_scale.pkl'\n",
    "F_YSCALER = f'{BASE}/y_scale.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caminho para os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FM_AIS = f'{BASE}/models_pop8_inte8/final_weights_ais_best_model.h5'\n",
    "FM_NSGAII = f'{BASE}/models_pop8_inte8/final_weights_nsgaii_best_model.h5'\n",
    "FM_PSO = f'{BASE}/models_pop8_inte8/final_weights_pso_best_model.h5'\n",
    "FM_RSEARCH = f'{BASE}/models_pop8_inte8/final_weights_randomsearch_best_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caminho para o History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HISTORY = f'{BASE}/models_pop8_inte8/history_nsgaii_best_model.pk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caminho para os dados brutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [train|test|val]_mundial_estadual[_OHE]_model[_peak]_20200629\n",
    "RAW_TRAIN = 'raw_datas/train_mundial_estadual_model_peak_20200731.csv'\n",
    "RAW_TEST = 'raw_datas/test_mundial_estadual_model_peak_20200731.csv'\n",
    "RAW_VAL = 'raw_datas/val_mundial_estadual_model_peak_20200731.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuração de look-back e look-forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LB = 25\n",
    "LF = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-processamento para simulação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando dados brutos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(RAW_TRAIN)\n",
    "test = pd.read_csv(RAW_TEST)\n",
    "val = pd.read_csv(RAW_VAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colunas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Colunas para usar no X\n",
    "x_columns = COLS_WD\n",
    "#Colunas para usar no Y\n",
    "y_columns = ['peak', 'postinflex', 'preinflex']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerando o x e y para treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = generate_regions_dataset(\n",
    "    key=\"Country/Region\",\n",
    "    regions_data=train,\n",
    "    look_back=LB,\n",
    "    look_forward=LF,\n",
    "    x_columns=x_columns,\n",
    "    y_columns=y_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerando o x e y para validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val, y_val = generate_regions_dataset(\n",
    "    key=\"Country/Region\",\n",
    "    regions_data=val,\n",
    "    look_back=LB,\n",
    "    look_forward=LF,\n",
    "    x_columns=x_columns,\n",
    "    y_columns=y_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerando o x e y para teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = generate_regions_dataset(\n",
    "    key=\"Country/Region\",\n",
    "    regions_data=test,\n",
    "    look_back=LB,\n",
    "    look_forward=LF,\n",
    "    x_columns=x_columns,\n",
    "    y_columns=y_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando colunas do shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = x_val.columns\n",
    "y_cols = y_val.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizando e armazenando o scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "x_train_scaled = x_scaler.fit_transform(x_train)\n",
    "x_test_scaled = x_scaler.fit_transform(x_test)\n",
    "x_val_scaled = x_scaler.transform(x_val)\n",
    "\n",
    "y_train_scaled = y_scaler.fit_transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)\n",
    "y_val_scaled = y_scaler.transform(y_val)\n",
    "\n",
    "# Descomente para salvar\n",
    "pickle.dump(x_scaler, open(F_XSCALER, 'wb'))\n",
    "pickle.dump(y_scaler, open(F_YSCALER, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armazenando estrutura para entrada no modelo\n",
    "\n",
    "_Necessário para rodar o modelo no HPC, opcional para o relatório_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(x_train_scaled, columns=x_cols).to_csv(F_XTRAIN, index=False)\n",
    "pd.DataFrame(y_train_scaled, columns=y_cols).to_csv(F_YTRAIN, index=False)\n",
    "\n",
    "pd.DataFrame(x_val_scaled, columns=x_cols).to_csv(F_XVAL, index=False)\n",
    "pd.DataFrame(y_val_scaled, columns=y_cols).to_csv(F_YVAL, index=False)\n",
    "\n",
    "# Descomente para salvar (em regra não será necessário salvar o teste)\n",
    "# pd.DataFrame(x_test_scaled, columns=x_cols).to_csv(F_XTEST, index=False)\n",
    "# pd.DataFrame(y_test_scaled, columns=y_cols).to_csv(F_YTEST, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando os dados\n",
    "\n",
    "_Opcional, caso já esteja carregado_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_scaled = pd.read_csv(F_XTRAIN)\n",
    "# x_test_scaled = pd.read_csv(F_XTEST)\n",
    "# x_val_scaled = pd.read_csv(F_XVAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando scaler e normalizando os dados\n",
    "\n",
    "_Opcional, caso já esteja carregado_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_scaler = pickle.load(open(F_XSCALER, 'rb'))\n",
    "# y_scaler = pickle.load(open(F_YSCALER, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = x_scaler.transform(x_train)\n",
    "x_test_scaled = x_scaler.transform(x_test)\n",
    "x_val_scaled = x_scaler.transform(x_val)\n",
    "\n",
    "y_train_scaled = y_scaler.transform(y_train)\n",
    "y_test_scaled = y_scaler.transform(y_test)\n",
    "y_val_scaled = y_scaler.transform(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model = keras.models.load_model(\n",
    "        FM_NSGAII, # FM_AIS, FM_NSGAII, FM_PSO, FM_RSEARCH\n",
    "        custom_objects={\n",
    "            'R_squared':ai_metrics.R_squared,\n",
    "            'pearson_r':ai_metrics.pearson_r,\n",
    "            'fac2':ai_metrics.fac2\n",
    "        }\n",
    "    )\n",
    "    print(\"Model found!\")\n",
    "except OSError:\n",
    "    print(\"Model not found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histórico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history_from_file(\n",
    "    HISTORY, f'{BASE}/history_{\"_\".join(y_columns)}_lb{LB}_lf{LF}.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados com todo conjunto de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = model.predict(x_train_scaled)\n",
    "y_train_pred = y_scaler.inverse_transform(y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = pd.DataFrame(y_train_pred, columns=y_cols)\n",
    "train_pred.to_csv(f'{BASE}/train_pred_{\"_\".join(y_columns)}_lb{LB}_lf{LF}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(y_train_pred, columns=y_cols)\n",
    "train_metrics = get_metrics(y_train, y_pred)\n",
    "train_metrics.to_csv(\n",
    "    f'{BASE}/train_metrics_{\"_\".join(y_columns)}_lb{LB}_lf{LF}.csv'\n",
    ")\n",
    "train_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_pred = model.predict(x_val_scaled)\n",
    "y_val_pred = y_scaler.inverse_transform(y_val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = pd.DataFrame(y_val_pred, columns=y_cols)\n",
    "val_pred.to_csv(f'{BASE}/val_pred_{\"_\".join(y_columns)}_lb{LB}_lf{LF}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(y_val_pred, columns=y_cols)\n",
    "val_metrics = get_metrics(y_val, y_pred)\n",
    "val_metrics.to_csv(\n",
    "    f'{BASE}/val_metrics_{\"_\".join(y_columns)}_lb{LB}_lf{LF}.csv'\n",
    ")\n",
    "val_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(x_test_scaled)\n",
    "y_test_pred = y_scaler.inverse_transform(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = pd.DataFrame(y_test_pred, columns=y_cols)\n",
    "test_pred.to_csv(f'{BASE}/test_pred_{\"_\".join(y_columns)}_lb{LB}_lf{LF}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(y_test_pred, columns=y_cols)\n",
    "test_metrics = get_metrics(y_test, y_pred)\n",
    "test_metrics.to_csv(\n",
    "    f'{BASE}/test_metrics_{\"_\".join(y_columns)}_lb{LB}_lf{LF}.csv'\n",
    ")\n",
    "test_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_metrics = {\n",
    "    'Treinamento': train_metrics,\n",
    "    'Validação': val_metrics,\n",
    "    'Teste': test_metrics\n",
    "}\n",
    "plot_metrics(\n",
    "    dict_metrics,\n",
    "    title='Métricas para cada horizonte de previsão do número de mortes em cada partição de dados',\n",
    "    out=f'{BASE}/metrics_{\"_\".join(y_columns)}_lb{LB}_lf{LF}.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avaliando para o LOCALE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCALE = 'Brazil'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = test[test['Country/Region'] == LOCALE].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativa adicionando o dia de pandemia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomente para utilizar\n",
    "# f1 = target['Country/Region'] == v\n",
    "# target.loc[f1, 'day'] = range(1,len(target[f1])+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerando o x e y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_target, y_target = generate_regions_dataset(\n",
    "    key=\"Country/Region\",\n",
    "    regions_data=target,\n",
    "    look_back=LB,\n",
    "    look_forward=LF,\n",
    "    x_columns=x_columns,\n",
    "    y_columns=y_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizando x para predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_target_scaled = x_scaler.transform(x_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predição e avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pred = model.predict(x_target_scaled)\n",
    "target_pred = y_scaler.inverse_transform(target_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame(target_pred, columns=y_cols)\n",
    "target_metrics = get_metrics(y_target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando o resultado da predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.to_csv(f'{BASE}/target_pred_{\"_\".join(y_columns)}_lb{LB}_lf{LF}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráfico com as métricas de predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_target = {LOCALE: target_metrics}\n",
    "plot_metrics(\n",
    "    dict_target,\n",
    "    title=f'Métricas para cada horizonte de previsão do mortes no {LOCALE}',\n",
    "    out=f'{BASE}/metrics_test_{\"_\".join(y_columns)}_lb{LB}_lf{LF}.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráficos de dispersão para cada horizonte de predição no Brasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter(\n",
    "    y_target,\n",
    "    y_pred,\n",
    "    title=f'Gráficos de dispersão do número mortes no {LOCALE}',\n",
    "    out=f'{BASE}/scatter_{\"_\".join(y_columns)}_lb{LB}_lf{LF}.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gráficos das previsões para cada horizonte de previsão no Brasil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples para peak, pre|post inflex (descomente para utilizar)\n",
    "# def plot_samples(y_true, y_pred, out, title='', xlabel='', ylabel=''):\n",
    "    \n",
    "#     plt.figure(figsize=(15, 5))\n",
    "#     plt.suptitle(title, fontsize=14, y=1.03)\n",
    "    \n",
    "#     n_cols = y_true.shape[1]\n",
    "    \n",
    "#     for col in range(n_cols):\n",
    "        \n",
    "#         days = list(range(col+1, (col+1)+len(y_true)))\n",
    "        \n",
    "#         plt.subplot((n_cols/4)+1, 3, col+1) # necessário alterar a quantidade de colunas dependendo do tamanho do target\n",
    "#         plt.title('Previsões para {}'.format(y_pred.columns[col]))\n",
    "#         plt.plot(days, y_true.iloc[:, col], label='Real')\n",
    "#         plt.plot(days, y_pred.iloc[:, col], label='Previsão')\n",
    "#         plt.legend()\n",
    "#         plt.xlabel(xlabel, fontsize=10)\n",
    "#         plt.ylabel(ylabel, fontsize=10)\n",
    "#         plt.xlim(left=days[0], right=days[-1])\n",
    "#         plt.xticks(np.linspace(days[0], days[-1], num=5, dtype=int))\n",
    "        \n",
    "#     plt.tight_layout()\n",
    "#     plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "#     plt.savefig(out)\n",
    "#     plt.show()\n",
    "\n",
    "plot_samples(\n",
    "    y_target,\n",
    "    y_pred,\n",
    "    title=f'Previsão do número de mortes no {LOCALE}',\n",
    "    xlabel='Dia',\n",
    "    ylabel='Quantidade de mortos',\n",
    "    out=f'{BASE}/predict_{\"_\".join(y_columns)}_lb{LB}_lf{LF}.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
