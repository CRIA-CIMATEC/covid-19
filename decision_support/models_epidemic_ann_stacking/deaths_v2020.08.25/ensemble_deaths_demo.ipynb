{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "from functools import partial\n",
    "from multiprocessing import Pool\n",
    "from collections import namedtuple\n",
    "from urllib.request import urlopen\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_single_region_dataset(key, region_data, look_back, look_forward, x_columns, y_columns=None, gen_x=True, gen_y=True):\n",
    "    \n",
    "    # Check region dataframe\n",
    "    if region_data is None:\n",
    "        print('generate_single_region_dataset error: Region data is None!')\n",
    "        return (None, None)\n",
    "    \n",
    "    # Check number of regions\n",
    "    if len(region_data[key].unique()) > 1:\n",
    "        print('generate_single_region_dataset error: More than one region in the dataframe!')\n",
    "        return (None, None)\n",
    "    else:\n",
    "        region_name = region_data[key].unique()[0]\n",
    "    \n",
    "    # Drop 'Region' column\n",
    "    region_data = region_data.drop(columns=key)\n",
    "    \n",
    "    # Check the number of samples available to\n",
    "    # generate the look back and look forward windows\n",
    "    if len(region_data) < (look_back + look_forward):\n",
    "        print('generate_single_region_dataset error: Not enough samples '+\n",
    "              'in {} to generate the windows!'.format(region_name))\n",
    "        return (None, None)\n",
    "    \n",
    "    n_samples = len(region_data) - look_back - look_forward + 1\n",
    "\n",
    "    var_names = x_columns\n",
    "    \n",
    "    # Generate inputs\n",
    "    if gen_x:\n",
    "        inputs = pd.DataFrame()\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            input_window = region_data.T.iloc[:, i:i+look_back]\n",
    "            wide_input_sample = pd.DataFrame()\n",
    "            \n",
    "            for var in var_names:\n",
    "                var_input_sample = input_window.loc[var:var, :]\n",
    "                var_input_sample.columns = ['{}_t{}'.format(var, a) for a in range(1-look_back, 1)]\n",
    "                var_input_sample = var_input_sample.reset_index(drop=True)\n",
    "                wide_input_sample = pd.concat([wide_input_sample, var_input_sample], axis='columns')\n",
    "                \n",
    "            inputs = pd.concat([inputs, wide_input_sample], axis='index')\n",
    "            \n",
    "        # Insert region name\n",
    "        #inputs.insert(loc=0, column=key, value=region_name)\n",
    "        # Reset index\n",
    "        inputs = inputs.reset_index(drop=True)\n",
    "\n",
    "    # Generate outputs\n",
    "    if gen_y:\n",
    "        \n",
    "        if y_columns is None:\n",
    "            print('generate_single_region_dataset error: Need to specify column labels!')\n",
    "            return (None, None)\n",
    "        \n",
    "        var_names = y_columns\n",
    "        outputs = pd.DataFrame()\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            output_window = region_data.T.iloc[:, i+look_back : i+look_back+look_forward]\n",
    "            wide_output_sample = pd.DataFrame()\n",
    "\n",
    "            for var in var_names:\n",
    "                var_output_sample = output_window.loc[var:var, :]\n",
    "                var_output_sample.columns = ['{}_t+{}'.format(var, a) for a in range(1, look_forward+1)]\n",
    "                var_output_sample = var_output_sample.reset_index(drop=True)\n",
    "                wide_output_sample = pd.concat([wide_output_sample, var_output_sample], axis='columns')\n",
    "\n",
    "            outputs = pd.concat([outputs, wide_output_sample], axis='index')\n",
    "        \n",
    "        # Insert region name\n",
    "        #outputs.insert(loc=0, column=key, value=region_name)\n",
    "        # Reset index\n",
    "        outputs = outputs.reset_index(drop=True)\n",
    "        \n",
    "    if gen_x and gen_y:\n",
    "        return (inputs, outputs)\n",
    "    elif gen_x:\n",
    "        return (inputs, None)\n",
    "    elif gen_y:\n",
    "        return (None, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_single_region_dataset(args, region_name):\n",
    "    \n",
    "    key = args[0]\n",
    "    regions_data = args[1]\n",
    "    \n",
    "    region_data = regions_data[regions_data[key]==region_name]\n",
    "    \n",
    "    new_args = args.copy()\n",
    "    new_args[1] = region_data\n",
    "    \n",
    "    region_x, region_y = generate_single_region_dataset(*new_args)\n",
    "    \n",
    "    return (region_x, region_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_regions_dataset(key, regions_data, look_back, look_forward, x_columns, y_columns=None, \n",
    "                             gen_x=True, gen_y=True):\n",
    "    \n",
    "    regions_names = regions_data[key].unique()\n",
    "    \n",
    "    all_regions_x = pd.DataFrame()\n",
    "    all_regions_y = pd.DataFrame()\n",
    "    \n",
    "    args = [key, regions_data, look_back, look_forward, x_columns, y_columns, gen_x, gen_y]\n",
    "    \n",
    "    func_gen_region_dataset = partial(\n",
    "        _get_single_region_dataset,\n",
    "        args\n",
    "    )\n",
    "    \n",
    "    n_jobs = os.cpu_count()\n",
    "    \n",
    "    chunk = max(1, (int(len(regions_names)/n_jobs)))\n",
    "    \n",
    "    with Pool(n_jobs) as pool:\n",
    "        datasets_list = pool.map(func_gen_region_dataset, regions_names, chunksize=chunk)\n",
    "        \n",
    "    for dataset in datasets_list:\n",
    "        \n",
    "        region_x = dataset[0]\n",
    "        region_y = dataset[1]\n",
    "        \n",
    "        if not (region_x is None):\n",
    "            all_regions_x = pd.concat([all_regions_x, region_x])\n",
    "            all_regions_x = all_regions_x.reset_index(drop=True)\n",
    "            \n",
    "        if not (region_y is None):\n",
    "            all_regions_y = pd.concat([all_regions_y, region_y])\n",
    "            all_regions_y = all_regions_y.reset_index(drop=True)\n",
    "    \n",
    "    if gen_x and gen_y:\n",
    "        return (all_regions_x, all_regions_y)\n",
    "    elif gen_x:\n",
    "        return (all_regions_x, None)\n",
    "    elif gen_y:\n",
    "        return (None, all_regions_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para carregamento dos dados de entrada dos modelos treinados com dados sintéticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_ID(df):\n",
    "    lista_regiao = list(df[\"Country/Region\"].unique())\n",
    "    identificadores = np.arange(0,len(lista_regiao))\n",
    "    lista_df = []\n",
    "    l=0\n",
    "    for i in lista_regiao:\n",
    "        \n",
    "        mask = df['Country/Region'] == i\n",
    "        df_temp = df.loc[mask]\n",
    "        df_temp.insert(0, \"ID\", identificadores[l] )\n",
    "        lista_df.append(df_temp)\n",
    "        l=l+1\n",
    "    df_final = pd.concat(lista_df) \n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_PT_multi(sequences, n_steps_in, n_steps_out):\n",
    "    #Função pare desenvolvimento dos preditores\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out-1\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix >= len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :-1], sequences[end_ix:out_end_ix+1, -1]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preditores_targets(np_treino, vp, vf):\n",
    "    identificador = np.unique(np_treino[:,0], axis=0)  \n",
    "    lista_treinoX = []\n",
    "    lista_treinoY = []\n",
    "    for i in identificador:\n",
    "        \n",
    "        mask = np_treino[:,0] ==i\n",
    "        dados_treino_temp = np_treino[mask]\n",
    "        dados_treino_temp = dados_treino_temp[:,1:]\n",
    "        \n",
    "        if(vp + vf <= dados_treino_temp.shape[0]):\n",
    "            treinamentoX_full, treinamentoY_full = create_PT_multi(dados_treino_temp, vp, vf) \n",
    "            lista_treinoX.append(treinamentoX_full)\n",
    "            lista_treinoY.append(treinamentoY_full)\n",
    "        else:\n",
    "            print(\"A serie temporal com identificação \"+ str(i) + \" não foi considerada (poucos dados)\")\n",
    "    \n",
    "    preditores = np.concatenate(lista_treinoX)\n",
    "    targets = np.concatenate(lista_treinoY)\n",
    "    \n",
    "    return preditores, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe para gerenciamento dos modelos de base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModelsDeaths:\n",
    "    \n",
    "    def __init__(self, base_path):\n",
    "        \n",
    "        self.load_models(base_path)\n",
    "    \n",
    "    def _load_artifacts(self, model_path):\n",
    "        \n",
    "        with open('{}/x_scaler.pkl'.format(model_path), 'rb') as scaler_file:\n",
    "            x_scaler = pickle.load(scaler_file)\n",
    "\n",
    "        with open('{}/y_scaler.pkl'.format(model_path), 'rb') as scaler_file:\n",
    "            y_scaler = pickle.load(scaler_file)\n",
    "\n",
    "        model = keras.models.load_model('{}/model.h5'.format(model_path))\n",
    "        \n",
    "        artifacts = {'x_scaler':x_scaler, 'y_scaler':y_scaler, 'model':model}\n",
    "        \n",
    "        artifacts = namedtuple('Artifacts', artifacts.keys())(*artifacts.values())\n",
    "        \n",
    "        return artifacts\n",
    "    \n",
    "    def load_models(self, base_path):\n",
    "        \n",
    "        model_paths = os.listdir(base_path)\n",
    "        \n",
    "        models = dict()\n",
    "        \n",
    "        for model_path in model_paths:\n",
    "            \n",
    "            model_artifacts = self._load_artifacts('{}/{}'.format(base_path,model_path))\n",
    "            models[model_path] = model_artifacts\n",
    "        \n",
    "        self.models = namedtuple('Models', models.keys())(*models.values())\n",
    "        \n",
    "    def transform_x_data(self, x_data, model_name):\n",
    "        \n",
    "        model_artifacts = getattr(self.models, model_name)\n",
    "        \n",
    "        x_data_scaled = model_artifacts.x_scaler.transform(x_data)\n",
    "        \n",
    "        return x_data_scaled\n",
    "        \n",
    "    def inverse_transform_y_data(self, y_data_scaled, model_name):\n",
    "        \n",
    "        model_artifacts = getattr(self.models, model_name)\n",
    "        \n",
    "        y_data = model_artifacts.y_scaler.inverse_transform(y_data_scaled)\n",
    "        \n",
    "        return y_data\n",
    "        \n",
    "    def reshape_x_data(self, x_data, model_name, lookback=4, lookforward=0):\n",
    "        \n",
    "        if model_name == 'CNN_LSTM_real':\n",
    "            x_columns = 10\n",
    "            \n",
    "            new_x_data = np.empty([x_data.shape[0], lookback, x_columns])\n",
    "            \n",
    "            k = 0\n",
    "            for i in range(x_columns):\n",
    "                for j in range(lookback):\n",
    "                    new_x_data[:, j, i] = x_data[:, k]\n",
    "                    k = k + 1\n",
    "        \n",
    "        elif model_name == 'LSTM_real':\n",
    "            \n",
    "            new_x_data = np.reshape(x_data, (x_data.shape[0], 1, x_data.shape[1]))\n",
    "            \n",
    "        elif (model_name=='LSTM_sintetico') or (model_name=='CNN_LSTM_sintetico'):\n",
    "            \n",
    "            new_x_data, _ = preditores_targets(x_data, lookback, lookforward)\n",
    "        \n",
    "        else:\n",
    "            print('BaseModelsDeaths.reshape_x_data error. Model name not defined.')\n",
    "            return None\n",
    "        \n",
    "        return new_x_data\n",
    "    \n",
    "    def model_predict(self, dataset, model_name, lookback=4, lookforward=0, scale=True):\n",
    "        \n",
    "        if (model_name=='LSTM_real') or (model_name=='CNN_LSTM_real'):\n",
    "            \n",
    "            x_columns = [\"Deaths\", \"Confirmed\", \"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\"]\n",
    "            y_columns = [\"Deaths\"]\n",
    "            \n",
    "            # Gera dados de entrada para o modelo com colunas de lookback\n",
    "            x_data, _ = generate_regions_dataset(\n",
    "                \"Country/Region\", dataset, lookback, lookforward, x_columns, y_columns=y_columns,\n",
    "                gen_x=True, gen_y=False\n",
    "            )\n",
    "                \n",
    "        elif (model_name=='LSTM_sintetico') or (model_name=='CNN_LSTM_sintetico'):\n",
    "            \n",
    "            # Insere coluna ID com IDs para cada região do DataFrame\n",
    "            dataset_id = input_ID(dataset)\n",
    "            \n",
    "            # Formatando colunas de interesse na ordem correta\n",
    "            x_columns = [\"ID\",\"Deaths\", \"Confirmed\", \"C1\",\"C2\",\"C3\",\"C4\",\"C5\",\"C6\",\"C7\",\"C8\"]\n",
    "            y_columns = [\"Deaths\"]\n",
    "            cols_sequence = x_columns + y_columns\n",
    "            x_data = dataset_id[cols_sequence]\n",
    "            \n",
    "        else:\n",
    "            print('BaseModelsDeaths.model_predict error. Model name not defined.')\n",
    "            return None\n",
    "        \n",
    "        x_data_scaled = self.transform_x_data(x_data, model_name)\n",
    "        \n",
    "        new_x_data = self.reshape_x_data(x_data_scaled, model_name, lookback, lookforward)\n",
    "        \n",
    "        model_artifacts = getattr(self.models, model_name)\n",
    "        y_pred = model_artifacts.model.predict(new_x_data)    \n",
    "        \n",
    "        if scale:\n",
    "            y_pred = self.inverse_transform_y_data(y_pred, model_name)\n",
    "        \n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, dataset, lookback=4, lookforward=0, scale=False):\n",
    "        \n",
    "        models_names = self.models._fields\n",
    "        n_models = len(models_names)\n",
    "        \n",
    "        models_preds = []\n",
    "        \n",
    "        for model_name in models_names:\n",
    "            \n",
    "            y_pred = self.model_predict(\n",
    "                dataset, model_name, lookback=lookback, lookforward=lookforward, scale=scale\n",
    "            )\n",
    "            \n",
    "            models_preds.append(y_pred)\n",
    "            \n",
    "        y_preds = np.hstack(models_preds)\n",
    "        \n",
    "        y_preds = np.reshape(y_preds, (y_preds.shape[0], 30, n_models), order='F')\n",
    "        \n",
    "        return y_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classe para gerenciamento do metamodelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaModelDeaths:\n",
    "    \n",
    "    def __init__(self, base_path, metamodel_path):\n",
    "        \n",
    "        self.base_models = BaseModelsDeaths(base_path)\n",
    "        \n",
    "        self.load_model(metamodel_path)\n",
    "        \n",
    "    def load_model(self, metamodel_path):\n",
    "        \n",
    "        with open('{}/meta_y_scaler.pkl'.format(metamodel_path), 'rb') as scaler_file:\n",
    "            self.meta_y_scaler = pickle.load(scaler_file)\n",
    "\n",
    "        self.metamodel = keras.models.load_model('{}/metamodel.h5'.format(metamodel_path))\n",
    "    \n",
    "    def predict(self, x_data, lookback=4, lookforward=0, scale=True):\n",
    "        \n",
    "        base_pred = self.base_models.predict(x_data, lookback=lookback, lookforward=lookforward)\n",
    "        \n",
    "        meta_pred = self.metamodel.predict(base_pred)\n",
    "        \n",
    "        if scale:\n",
    "            meta_pred = self.meta_y_scaler.inverse_transform(meta_pred)\n",
    "        \n",
    "        return meta_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando dados atualizados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando arquivo CSV com dataset atualizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_dataset = pd.read_csv('../dataset/complete_20200821.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando dados atualizados a partir de endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://covid19.fieb.org.br:9050/api/return/calculate/cases/?country=Brazil\"\n",
    "response = urlopen(url)\n",
    "request = json.loads(response.read())\n",
    "dataframe = pd.DataFrame(request['Cases'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modificando DataFrame gerado com dados do endpoint para formato esperado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['c{}1valor'.format(i) for i in range(1,9)]\n",
    "cols_to_drop.append('recovered')\n",
    "dataframe = dataframe.drop(columns=cols_to_drop)\n",
    "dataframe.columns = [\n",
    "    'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8',\n",
    "    'Confirmed', 'Deaths', 'Date', 'Country/Region'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando metamodelo para fazer previsões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucas/miniconda3/envs/covid-web/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator MinMaxScaler from version 0.22.2.post1 when using version 0.23.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "metamodel = MetaModelDeaths('base_models', 'CNN_LSTM_metamodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criando dataframe para a Bahia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bahia_endpoint = dataframe[dataframe['Country/Region']=='Bahia'].copy()\n",
    "bahia_dataset = complete_dataset[complete_dataset['Country/Region']=='BA'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminando últimos dados dos DataFrames apenas para fazer a comparação das previsões."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bahia_endpoint = bahia_endpoint.iloc[:-43, :]\n",
    "bahia_dataset = bahia_dataset.iloc[:-38, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo possui lookback de 4 dias. Selecionando apenas últimos 4 dias dos dados para entrar no modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_bahia_endpoint = bahia_endpoint.tail(4)\n",
    "last_bahia_dataset = bahia_dataset.tail(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerando previsões do metamodelo para o conjunto de dados da Bahia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_metamodel_endpoint = metamodel.predict(last_bahia_endpoint)\n",
    "pred_metamodel_dataset = metamodel.predict(last_bahia_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previsões para os próximos 30 dias usando dados do endpoint na entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.765167</td>\n",
       "      <td>47.473778</td>\n",
       "      <td>48.893234</td>\n",
       "      <td>54.261185</td>\n",
       "      <td>53.254818</td>\n",
       "      <td>49.888027</td>\n",
       "      <td>56.788425</td>\n",
       "      <td>55.138222</td>\n",
       "      <td>59.242661</td>\n",
       "      <td>61.555618</td>\n",
       "      <td>...</td>\n",
       "      <td>62.96949</td>\n",
       "      <td>65.91568</td>\n",
       "      <td>64.428535</td>\n",
       "      <td>61.30611</td>\n",
       "      <td>62.507931</td>\n",
       "      <td>65.645279</td>\n",
       "      <td>67.06739</td>\n",
       "      <td>64.187859</td>\n",
       "      <td>64.163872</td>\n",
       "      <td>68.330368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0          1          2          3          4          5   \\\n",
       "0  43.765167  47.473778  48.893234  54.261185  53.254818  49.888027   \n",
       "\n",
       "          6          7          8          9   ...        20        21  \\\n",
       "0  56.788425  55.138222  59.242661  61.555618  ...  62.96949  65.91568   \n",
       "\n",
       "          22        23         24         25        26         27         28  \\\n",
       "0  64.428535  61.30611  62.507931  65.645279  67.06739  64.187859  64.163872   \n",
       "\n",
       "          29  \n",
       "0  68.330368  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_endpoint = pd.DataFrame(pred_metamodel_endpoint)\n",
    "display(pred_endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previsões para os próximos 30 dias usando dados do dataset na entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43.765167</td>\n",
       "      <td>47.473778</td>\n",
       "      <td>48.893234</td>\n",
       "      <td>54.261185</td>\n",
       "      <td>53.254818</td>\n",
       "      <td>49.888027</td>\n",
       "      <td>56.788425</td>\n",
       "      <td>55.138222</td>\n",
       "      <td>59.242661</td>\n",
       "      <td>61.555618</td>\n",
       "      <td>...</td>\n",
       "      <td>62.96949</td>\n",
       "      <td>65.91568</td>\n",
       "      <td>64.428535</td>\n",
       "      <td>61.30611</td>\n",
       "      <td>62.507931</td>\n",
       "      <td>65.645279</td>\n",
       "      <td>67.06739</td>\n",
       "      <td>64.187859</td>\n",
       "      <td>64.163872</td>\n",
       "      <td>68.330368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0          1          2          3          4          5   \\\n",
       "0  43.765167  47.473778  48.893234  54.261185  53.254818  49.888027   \n",
       "\n",
       "          6          7          8          9   ...        20        21  \\\n",
       "0  56.788425  55.138222  59.242661  61.555618  ...  62.96949  65.91568   \n",
       "\n",
       "          22        23         24         25        26         27         28  \\\n",
       "0  64.428535  61.30611  62.507931  65.645279  67.06739  64.187859  64.163872   \n",
       "\n",
       "          29  \n",
       "0  68.330368  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pred_dataset = pd.DataFrame(pred_metamodel_dataset)\n",
    "display(pred_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verificando se previsões são iguais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_endpoint.equals(pred_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previsões na escala normalizada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01678484, 0.01817539, 0.01870762, 0.02072035, 0.02034301,\n",
       "        0.01908062, 0.02166795, 0.0210492 , 0.02258817, 0.01608941,\n",
       "        0.01284448, 0.01555829, 0.01416678, 0.01531472, 0.01659657,\n",
       "        0.01687943, 0.01537516, 0.01591045, 0.01692881, 0.01691584,\n",
       "        0.01645306, 0.01721082, 0.01682833, 0.01602523, 0.01633434,\n",
       "        0.01714128, 0.01750704, 0.01676643, 0.01676025, 0.01783188]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metamodel.predict(last_bahia_endpoint, scale=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
