{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datetime as dt\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alterar='20200908'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JHU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv\"\n",
    "df_confirmed = pd.read_csv(url, error_bad_lines=False,parse_dates=True)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv\"\n",
    "df_death = pd.read_csv(url, error_bad_lines=False,parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alteração nos dados baseados nas correções dos reportes oficiais\n",
    "\n",
    "O reporte do dia 17/02/20 (referente aos dados de 16/02/20) diz que a partir de então, os dados serão confirmados pelos exames laboratoriais e clínicos (antes era apenas laobatorial- apenas Hubei). Com isso, atualizamos os dados de 12 a 14/02 (que tem esses dados disponiveis) \n",
    "\n",
    "https://www.who.int/docs/default-source/coronaviruse/situation-reports/20200217-sitrep-28-covid-19.pdf?sfvrsn=a19cf2ad_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# função para alterar valor\n",
    "def change_val(date, ref_col, dataset,dtnry):\n",
    "    for key, val in dtnry.items():\n",
    "        dataset[date].loc[(dataset[ref_col]==key)]=val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_11_conf = {'Hubei' : 33366}\n",
    "feb_12_conf = {'Hubei' : 34874}\n",
    "feb_13_conf = {'Hubei' : 51986}\n",
    "feb_14_conf = {'Hubei' : 54406}\n",
    "       \n",
    "change_val(\"2/12/20\",\"Province/State\", df_confirmed,feb_12_conf)\n",
    "change_val(\"2/13/20\",\"Province/State\",df_confirmed,feb_13_conf)\n",
    "change_val(\"2/14/20\",\"Province/State\",df_confirmed,feb_14_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando dataset diário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_serie(dataframe : pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    df = dataframe.copy()\n",
    "    df_diff = df.iloc[:,4:]\n",
    "    df_diff = df_diff.T.diff().T\n",
    "    df_diff.mask(df_diff < 0, 0,inplace=True)\n",
    "    df_diff=pd.concat([df.iloc[:,0:4],df_diff.iloc[:,1:]], axis=1)\n",
    "          \n",
    "    return df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_death1=diff_serie(df_death)\n",
    "df_confirmed1=diff_serie(df_confirmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset diário no formato long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates=df_confirmed1.columns[4:]\n",
    "\n",
    "confirmed_long = df_confirmed1.melt(id_vars=['Province/State', 'Country/Region'], \n",
    "                            value_vars=dates, var_name='Date', value_name='Confirmed')\n",
    "\n",
    "death_long = df_death1.melt(id_vars=['Province/State', 'Country/Region'], \n",
    "                            value_vars=dates, var_name='Date', value_name='Deaths')\n",
    "\n",
    "full_long=pd.merge(confirmed_long, death_long,on=[\"Country/Region\",\"Province/State\",\"Date\"], how='left')\n",
    "\n",
    "full_long[\"Date\"]= pd.to_datetime(full_long[\"Date\"])\n",
    "full_long[\"Date\"] = full_long[\"Date\"].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "# Corrigindo a nomeclatura de alguns paises\n",
    "full_long['Country/Region'] = full_long['Country/Region'].replace('Korea, South', 'South Korea')\n",
    "full_long['Country/Region'] = full_long['Country/Region'].replace('Taiwan*', 'Taiwan')\n",
    "full_long['Country/Region'] = full_long['Country/Region'].replace('US', 'United States')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agrupando dataset por país"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=full_long.groupby([\"Date\",'Country/Region'])[[\"Date\",\"Confirmed\",\"Deaths\"]].sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Confirmed</th>\n",
       "      <th>Deaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01/23/2020</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01/23/2020</td>\n",
       "      <td>Albania</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01/23/2020</td>\n",
       "      <td>Algeria</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01/23/2020</td>\n",
       "      <td>Andorra</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01/23/2020</td>\n",
       "      <td>Angola</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43047</th>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>West Bank and Gaza</td>\n",
       "      <td>652.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43048</th>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>Western Sahara</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43049</th>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>Yemen</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43050</th>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>Zambia</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43051</th>\n",
       "      <td>09/07/2020</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>461.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>43052 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date      Country/Region  Confirmed  Deaths\n",
       "0      01/23/2020         Afghanistan        0.0     0.0\n",
       "1      01/23/2020             Albania        0.0     0.0\n",
       "2      01/23/2020             Algeria        0.0     0.0\n",
       "3      01/23/2020             Andorra        0.0     0.0\n",
       "4      01/23/2020              Angola        0.0     0.0\n",
       "...           ...                 ...        ...     ...\n",
       "43047  09/07/2020  West Bank and Gaza      652.0     3.0\n",
       "43048  09/07/2020      Western Sahara        0.0     0.0\n",
       "43049  09/07/2020               Yemen        2.0     1.0\n",
       "43050  09/07/2020              Zambia       60.0     0.0\n",
       "43051  09/07/2020            Zimbabwe      461.0     4.0\n",
       "\n",
       "[43052 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diluindo os casos clinicos da China (pico de 15410 em 12/02/20) nos dias anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feb_13_conf = {'China' : 1998}   \n",
    "df.loc[(df[\"Country/Region\"]==\"China\")& (df[\"Date\"]=='02/13/2020') , \"Confirmed\"] =1998\n",
    "soma=df[df[\"Country/Region\"]==\"China\"][0:22].groupby(['Country/Region'])[\"Confirmed\"].sum()[0]\n",
    "\n",
    "\n",
    "_f1 = df['Country/Region'] == 'China'\n",
    "_f2 = df['Date'] <= '02/13/2020'\n",
    "df.loc[_f1 & _f2, 'Confirmed'] += round(df.loc[_f1 & _f2, 'Confirmed']*15410/soma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset oxford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c1_schoolclosing.csv\"\n",
    "c1 = pd.read_csv(url, error_bad_lines=False,parse_dates=True, sep=\",\", encoding=\"latin\",skipfooter=3, na_values=['.'])\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c2_workplaceclosing.csv\"\n",
    "c2 = pd.read_csv(url, error_bad_lines=False,parse_dates=True, sep=\",\", encoding=\"latin\",skipfooter=3, na_values=['.'])\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c3_cancelpublicevents.csv\"\n",
    "c3 = pd.read_csv(url, error_bad_lines=False,parse_dates=True, sep=\",\", encoding=\"latin\",skipfooter=3, na_values=['.'])\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c4_restrictionsongatherings.csv\"\n",
    "c4 = pd.read_csv(url, error_bad_lines=False,parse_dates=True, sep=\",\", encoding=\"latin\",skipfooter=3, na_values=['.'])\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c5_closepublictransport.csv\"\n",
    "c5 = pd.read_csv(url, error_bad_lines=False,parse_dates=True, sep=\",\", encoding=\"latin\",skipfooter=3, na_values=['.'])\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c6_stayathomerequirements.csv\"\n",
    "c6 = pd.read_csv(url, error_bad_lines=False,parse_dates=True, sep=\",\", encoding=\"latin\",skipfooter=3, na_values=['.'])\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c7_domestictravel.csv\"\n",
    "c7 = pd.read_csv(url, error_bad_lines=False,parse_dates=True, sep=\",\", encoding=\"latin\",skipfooter=3, na_values=['.'])\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/OxCGRT/covid-policy-tracker/master/data/timeseries/c8_internationaltravel.csv\"\n",
    "c8 = pd.read_csv(url, error_bad_lines=False,parse_dates=True, sep=\",\", encoding=\"latin\",skipfooter=3, na_values=['.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates=c1.columns[2:]\n",
    "\n",
    "df_c1 = c1.melt(id_vars=['Unnamed: 0', 'Unnamed: 1'], \n",
    "                            value_vars=dates, var_name='Date', value_name='C1')\n",
    "df_c2 = c2.melt(id_vars=['Unnamed: 0', 'Unnamed: 1'], \n",
    "                            value_vars=dates, var_name='Date', value_name='C2')\n",
    "df_c3 = c3.melt(id_vars=['Unnamed: 0', 'Unnamed: 1'], \n",
    "                            value_vars=dates, var_name='Date', value_name='C3')\n",
    "df_c4 = c4.melt(id_vars=['Unnamed: 0', 'Unnamed: 1'], \n",
    "                            value_vars=dates, var_name='Date', value_name='C4')\n",
    "df_c5 = c5.melt(id_vars=['Unnamed: 0', 'Unnamed: 1'], \n",
    "                            value_vars=dates, var_name='Date', value_name='C5')\n",
    "df_c6 = c6.melt(id_vars=['Unnamed: 0', 'Unnamed: 1'], \n",
    "                            value_vars=dates, var_name='Date', value_name='C6')\n",
    "df_c7 = c7.melt(id_vars=['Unnamed: 0', 'Unnamed: 1'], \n",
    "                            value_vars=dates, var_name='Date', value_name='C7')\n",
    "df_c8 = c8.melt(id_vars=['Unnamed: 0', 'Unnamed: 1'], \n",
    "                            value_vars=dates, var_name='Date', value_name='C8')\n",
    "\n",
    "\n",
    "full=pd.merge(df_c1,df_c2,on=[\"Unnamed: 0\",\"Unnamed: 1\",\"Date\"], how='left')\n",
    "full=pd.merge(full,df_c3,on=[\"Unnamed: 0\",\"Unnamed: 1\",\"Date\"], how='left')\n",
    "full=pd.merge(full,df_c4,on=[\"Unnamed: 0\",\"Unnamed: 1\",\"Date\"], how='left')\n",
    "full=pd.merge(full,df_c5,on=[\"Unnamed: 0\",\"Unnamed: 1\",\"Date\"], how='left')\n",
    "full=pd.merge(full,df_c6,on=[\"Unnamed: 0\",\"Unnamed: 1\",\"Date\"], how='left')\n",
    "full=pd.merge(full,df_c7,on=[\"Unnamed: 0\",\"Unnamed: 1\",\"Date\"], how='left')\n",
    "full_long_mit=pd.merge(full,df_c8,on=[\"Unnamed: 0\",\"Unnamed: 1\",\"Date\"], how='left')\n",
    "\n",
    "full_long_mit[\"Date\"]= pd.to_datetime(full_long_mit[\"Date\"])\n",
    "full_long_mit[\"Date\"] = full_long_mit[\"Date\"].dt.strftime('%m/%d/%Y')\n",
    "\n",
    "full_long_mit.rename(columns={'Unnamed: 0': 'Country/Region', \"Unnamed: 1\":'COD'}, inplace=True)\n",
    "full_long_mit=full_long_mit.drop(columns=\"COD\")\n",
    "\n",
    "full_long_mit['Country/Region'] = full_long_mit['Country/Region'].replace( 'Cape Verde', 'Cabo Verde')\n",
    "full_long_mit['Country/Region'] = full_long_mit['Country/Region'].replace(  'Democratic Republic of Congo', 'Congo (Kinshasa)')\n",
    "full_long_mit['Country/Region'] = full_long_mit['Country/Region'].replace( 'Congo',  'Congo (Brazzaville)')\n",
    "full_long_mit['Country/Region'] = full_long_mit['Country/Region'].replace(  'Kyrgyz Republic',  'Kyrgyzstan')\n",
    "full_long_mit['Country/Region'] = full_long_mit['Country/Region'].replace(  'Slovak Republic', 'Slovakia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country/Region</th>\n",
       "      <th>Date</th>\n",
       "      <th>C1</th>\n",
       "      <th>C2</th>\n",
       "      <th>C3</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>C7</th>\n",
       "      <th>C8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aruba</td>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Angola</td>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Anguilla</td>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Albania</td>\n",
       "      <td>01/01/2020</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Country/Region        Date   C1   C2   C3   C4   C5   C6   C7   C8\n",
       "0          Aruba  01/01/2020  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "1    Afghanistan  01/01/2020  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "2         Angola  01/01/2020  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "3       Anguilla  01/01/2020  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "4        Albania  01/01/2020  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_long_mit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd=full_long_mit.copy()\n",
    "bd[\"C1\"].loc[(bd[\"C1\"]>0)]=1\n",
    "bd[\"C2\"].loc[(bd[\"C2\"]>0)]=1\n",
    "bd[\"C3\"].loc[(bd[\"C3\"]>0)]=1\n",
    "bd[\"C4\"].loc[(bd[\"C4\"]>0)]=1\n",
    "bd[\"C5\"].loc[(bd[\"C5\"]>0)]=1\n",
    "bd[\"C6\"].loc[(bd[\"C6\"]>0)]=1\n",
    "bd[\"C7\"].loc[(bd[\"C7\"]>0)]=1\n",
    "bd[\"C8\"].loc[(bd[\"C8\"]>0)]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete=pd.merge(df,bd, how=\"outer\").sort_values(\"Date\").reset_index(drop=True)\n",
    "df_complete=pd.merge(df,bd,on=[\"Country/Region\",\"Date\"], how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notas fiscais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo tabela com os dados\n",
    "sefaz_df = pd.read_csv('http://receitadados.fazenda.rs.gov.br/Documentos%20Compartilhados/Valor%20e%20quantidade%20de%20DFe%20por%20dia.csv',\n",
    "                       sep=';', encoding='latin-1', decimal=',',\n",
    "                       parse_dates=['Data', 'Dados Atualizados Até']\n",
    "                      )\n",
    "\n",
    "# Eliminando colunas sem utilidade\n",
    "sefaz_df = sefaz_df.drop(columns=['Ano', 'Mês', 'Tipo Emissão', 'Dados Atualizados Até'])\n",
    "\n",
    "# Selecionando apenas amostras de 2019 em diante\n",
    "sefaz_df = sefaz_df.loc[sefaz_df['Data'].dt.year >= 2019]\n",
    "\n",
    "# Selecionando apenas amostras de NFC-e\n",
    "sefaz_nfc_df = sefaz_df.loc[(sefaz_df['Modelo'] == 'NFC-e')]\n",
    "\n",
    "# Eliminando coluna de modelo\n",
    "sefaz_nfc_df = sefaz_nfc_df.drop(columns=['Modelo'])\n",
    "\n",
    "# Ordenando valores pela Data\n",
    "sefaz_nfc_df = sefaz_nfc_df.sort_values(by='Data')\n",
    "\n",
    "# Substituindo nomes dos estados pelas siglas\n",
    "states_alias = {'RIO GRANDE DO NORTE':'RN', 'SERGIPE':'SE', 'ESPIRITO SANTO':'ES',\n",
    "                'RIO DE JANEIRO':'RJ', 'RONDONIA':'RO', 'ALAGOAS':'AL', 'RORAIMA':'RR',\n",
    "                'BAHIA':'BA', 'RIO GRANDE DO SUL':'RS', 'ACRE':'AC', 'PIAUI':'PI', 'PARAIBA':'PB'}\n",
    "\n",
    "sefaz_nfc_df['UF'] = sefaz_nfc_df['UF'].replace(states_alias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sefaz_df_teste=sefaz_nfc_df.copy()\n",
    "sefaz_df_teste=sefaz_df_teste.loc[sefaz_df_teste['Data'].dt.year == 2019]\n",
    "sefaz_mean = sefaz_df_teste.groupby('UF').mean()\n",
    "sefaz_mean = sefaz_mean.add_suffix('_2019_mean')\n",
    "sefaz_mean = sefaz_mean.reset_index()\n",
    "sefaz_nfc_df = sefaz_nfc_df.merge(sefaz_mean, on='UF')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Casos confirmados brasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    'Host': 'xx9p7hp1p7.execute-api.us-east-1.amazonaws.com',\n",
    "    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:77.0) Gecko/20100101 Firefox/77.0',\n",
    "    'Accept': 'application/json, text/plain, */*',\n",
    "    'Accept-Language': 'en-US,en;q=0.5',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'X-Parse-Application-Id': 'unAFkcaNDeXajurGB7LChj8SgQYS2ptm',\n",
    "    'Origin': 'https://covid.saude.gov.br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Referer': 'https://covid.saude.gov.br/',\n",
    "    'TE': 'Trailers',\n",
    "    }\n",
    "json_with_url = requests.get('https://xx9p7hp1p7.execute-api.us-east-1.amazonaws.com/prod/PortalGeral', headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_covid_url = json_with_url.json().get('results')[0].get('arquivo').get('url')\n",
    "# covid_br_path = '../covid_br_data/HIST_PAINEL_COVIDBR_16jun2020.xlsx'\n",
    "\n",
    "covid_br_df = pd.read_excel(daily_covid_url, parse_dates=['data'])\n",
    "\n",
    "# Dicionário com estados que possuem dados de notas fiscais\n",
    "states_alias = {'RIO GRANDE DO NORTE':'RN', 'SERGIPE':'SE', 'ESPIRITO SANTO':'ES',\n",
    "                'RIO DE JANEIRO':'RJ', 'RONDONIA':'RO', 'ALAGOAS':'AL', 'RORAIMA':'RR',\n",
    "                'BAHIA':'BA', 'RIO GRANDE DO SUL':'RS', 'ACRE':'AC', 'PIAUI':'PI', 'PARAIBA':'PB'}\n",
    "\n",
    "# Selecionando apenas estados que possuem dados de notas fiscais\n",
    "covid_br_df = covid_br_df[covid_br_df['estado'].isin(states_alias.values())]\n",
    "\n",
    "# Selecionando dados relativos aos estados, desconsiderando municípios\n",
    "covid_br_df = covid_br_df[covid_br_df['codmun'].isna()]\n",
    "\n",
    "# Eliminando colunas sem utilidade\n",
    "covid_br_df = covid_br_df.drop(\n",
    "    columns=['regiao', 'municipio', 'coduf', 'codmun', 'codRegiaoSaude', 'nomeRegiaoSaude', 'semanaEpi',\n",
    "             'Recuperadosnovos', 'emAcompanhamentoNovos', 'interior/metropolitana', 'casosAcumulado', 'obitosAcumulado']\n",
    ")\n",
    "\n",
    "# Modificando nome das colunas\n",
    "covid_br_df.columns = ['UF', 'Data', 'populacao',\n",
    "                       'casosNovos', 'obitosNovos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "UFs = set(covid_br_df.UF)\n",
    "\n",
    "covid_br_new = pd.DataFrame(columns=covid_br_df.columns)\n",
    "\n",
    "for uf in UFs:\n",
    "    estado_df = covid_br_df[covid_br_df['UF'] == uf]\n",
    "\n",
    "    pop = estado_df.loc[:, 'populacao'].iloc[0]\n",
    "\n",
    "    for i in range(15, 25):\n",
    "        new_row = {\n",
    "            'UF': uf, \n",
    "            'Data': f'{i}/02/2020',\n",
    "            'populacao': pop, \n",
    "            \n",
    "            'casosNovos': 0, \n",
    "            \n",
    "            'obitosNovos': 0\n",
    "        }\n",
    "        estado_df = estado_df.append(new_row, ignore_index=True)\n",
    "    \n",
    "    estado_df['Data'] = pd.to_datetime(estado_df['Data'])\n",
    "    covid_br_new = covid_br_new.append(estado_df, ignore_index=True)\n",
    "\n",
    "covid_br_new = covid_br_new.sort_values(by=['UF', 'Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = sefaz_nfc_df.merge(covid_br_new, how='left', on=['UF', 'Data'], validate='1:1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mitigação nacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "mitigacao_path = './brazil_states_policies_dataset/'\n",
    "df_mitigacao = pd.read_csv(mitigacao_path+'classificacoes_all_'+alterar+'.csv', sep=';', encoding='utf-16')\n",
    "\n",
    "# adicionando dias zerados no dataset para fazer merge\n",
    "for i in range(15, 25):\n",
    "    df_mitigacao[f'{i}/02/2020'] = 0\n",
    "\n",
    "dates = df_mitigacao.columns[2:]\n",
    "df_mitigacao = df_mitigacao.melt(\n",
    "        id_vars=['Classificacao', 'StateCode'],\n",
    "        value_vars=dates,\n",
    "        var_name='Date',\n",
    "        value_name='Mitigacao',\n",
    "    )\n",
    "\n",
    "df_mitigacao = df_mitigacao.pivot_table(index=['StateCode', 'Date'], columns='Classificacao', fill_value=0)\n",
    "\n",
    "df_mitigacao.columns = measures = df_mitigacao.columns.droplevel()\n",
    "df_mitigacao = df_mitigacao.reset_index()\n",
    "df_mitigacao.columns = ['UF', 'Data'] + list(measures)\n",
    "\n",
    "df_mitigacao['Data'] = pd.to_datetime(df_mitigacao['Data'], format='%d/%m/%Y')\n",
    "df_mitigacao = df_mitigacao.sort_values(by=['UF', 'Data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = pd.merge(full_dataset, df_mitigacao, on=['Data', 'UF'], how='inner').sort_values(by=['UF', 'Data'])\n",
    "uf_population = full_dataset.loc[:, ['UF','populacao']].drop_duplicates().dropna()\n",
    "\n",
    "for UF, population in zip(uf_population['UF'], uf_population['populacao']):\n",
    "    full_dataset.loc[full_dataset['UF']==UF, 'populacao'] = population\n",
    "    \n",
    "full_dataset = full_dataset.fillna(value=0)\n",
    "full_dataset = full_dataset.astype({'Qtde Autorizados':float})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = full_dataset.set_index('Data')\n",
    "full_dataset.insert(1, 'day_of_week', full_dataset.index.dayofweek) # adição do dia da semana no dataset\n",
    "full_dataset.reset_index(inplace=True)\n",
    "\n",
    "full_dataset.rename(columns={'UF': 'Country/Region', \"Data\":'Date', \"casosNovos\":\"Confirmed\", \"obitosNovos\":\"Deaths\"}, inplace=True)\n",
    "\n",
    "full_dataset[\"Date\"]= pd.to_datetime(full_dataset[\"Date\"])\n",
    "full_dataset[\"Date\"] = full_dataset[\"Date\"].dt.strftime('%m/%d/%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final=pd.concat([df_complete,full_dataset],axis=0, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_final.copy()\n",
    "country=df[\"Country/Region\"].unique()\n",
    "df_model = pd.DataFrame([])\n",
    "\n",
    "for uf in country:\n",
    "    \n",
    "    df1 = df[df['Country/Region']==uf].reset_index(drop=True)\n",
    "    index=df1[df1[\"Confirmed\"]>0].first_valid_index()\n",
    "    data=df1.iloc[index:]\n",
    "    df_model=df_model.append(data)\n",
    "\n",
    "df_model=df_model.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.to_csv('data/dados_gerais_'+ alterar +'.csv', index=False)\n",
    "df_complete.to_csv('data/dados_mundiais_'+ alterar +'.csv', index=False)\n",
    "full_dataset.to_csv('data/dados_nacionais_'+ alterar+'.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
